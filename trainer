#!/usr/bin/env python3

import sys
import numpy as np
from Parser import Board, DataParser
import pickle
import argparse

class ArgumentParser:
    def __init__(self):
        self.parser = argparse.ArgumentParser(description="Chess neural network")
        self.parser.add_argument("dataset", help="The dataset to use for training")
        self.parser.add_argument("-t", "--eta", help="Learning rate", type=float, default=0.1)
        self.parser.add_argument("-e", "--epochs", help="Number of epochs", type=int, default=5000)
        self.parser.add_argument("-b", "--batch", help="Batch size", type=int, default=10)
        self.parser.add_argument("-l", "--layers", help="Layers of the network", type=int, nargs="+", default=[64, 32, 16, 8, 4])
        self.parser.add_argument("-m", "--model", help="Model to load", type=str, required=False)

    def parse(self):
        return self.parser.parse_args()

class Network:
    def __init__(self, layers):
        self.layers = layers
        self.nbLayers = len(layers)
        # Generate random biases, except for the first layer (input layer)
        self.bias = [np.random.randn(y, 1) for y in layers[1:]]
        # Generate random weights for each layer, except for the first one (input layer)
        # The weights are generated as a matrix of size (y, x) where y is the number of nodes in the current layer
        # and x is the number of nodes in the previous layer
        self.weights = [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]

    def train(self, parser: DataParser, epochs: int, learningRate: float, miniBatchSize: int):
        for epoch in range(epochs):
            miniBatch = parser.makeRandomBatch(miniBatchSize)
            self.updateMiniBatch(miniBatch, learningRate)
        print("Training complete")
        print("Saving model...")
        self.saveToFile("model")

    def updateMiniBatch(self, miniBatch, learningRate):
        # Initialize the biases and weights gradients
        gradientB = [np.zeros(b.shape) for b in self.bias]
        gradientW = [np.zeros(w.shape) for w in self.weights]
        for board in miniBatch:
            # Compute the gradients for the current board
            deltaGradientB, deltaGradientW = self.backPropagation(board)
            # Add the gradients to the total gradients
            gradientB = [gb + dgb for gb, dgb in zip(gradientB, deltaGradientB)]
            gradientW = [gw + dgw for gw, dgw in zip(gradientW, deltaGradientW)]
        # Update the weights and biases
        self.weights = [w - (learningRate / len(miniBatch)) * gw for w, gw in zip(self.weights, gradientW)]
        self.bias = [b - (learningRate / len(miniBatch)) * gb for b, gb in zip(self.bias, gradientB)]

    def backPropagation(self, board: Board):
        # x is the input board
        # y is the expected output ==> board.res
        boardInt = board.toVector()
        currentLayer = 1
        # Initialize the biases and weights gradients
        gradientB = [np.zeros(b.shape) for b in self.bias]
        gradientW = [np.zeros(w.shape) for w in self.weights]
        zList = []
        nodeValues = [boardInt]

        nodeValue = self.feedForward(boardInt, zList, nodeValues)
        delta = self.costDerivative(nodeValue, board.res) * self.sigmoidPrime(zList[-currentLayer])
        gradientB[-currentLayer] = delta
        gradientW[-currentLayer] = np.dot(delta, nodeValues[-currentLayer - 1].transpose())

        for currentLayer in range(2, self.nbLayers):
            z = zList[-currentLayer]
            sp = self.sigmoidPrime(z)
            delta = np.dot(self.weights[-currentLayer + 1].transpose(), delta) * sp
            gradientB[-currentLayer] = delta
            gradientW[-currentLayer] = np.dot(delta, (nodeValues[-currentLayer - 1]).transpose())
        return (gradientB, gradientW)

    def feedForward(self, board, zList, nodeValues):
        for b, w in zip(self.bias, self.weights):
            z = np.dot(w, board) + b
            zList.append(z)
            board = self.sigmoid(z)
            nodeValues.append(board)
        return board

    def costDerivative(self, outputActivations, y):
        for i in range(len(outputActivations)):
            outputActivations[i] -= y[i]
        return outputActivations

    def sigmoid(self, z):
        return 1.0 / (1.0 + np.exp(-z))

    def sigmoidPrime(self, z):
        return self.sigmoid(z) * (1 - self.sigmoid(z))

    def feedForwardTest(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.bias, self.weights):
            a = self.sigmoid(np.dot(w, a)+b)
        return a

    def saveToFile(self, filename):
        with open(filename, "wb") as file:
            pickle.dump(self.layers, file)
            pickle.dump(self.weights, file)
            pickle.dump(self.bias, file)

    def loadModel(self, filename):
        with open(filename, "rb") as file:
            self.layers = pickle.load(file)
            self.weights = pickle.load(file)
            self.biais = pickle.load(file)

def main(parser, args):
    print("Initializing network...")
    print("Layers: {}".format(args.layers))
    network = Network(args.layers)
    if args.model:
        network.loadModel(args.model)
    print("Training...")
    network.train(parser, args.epochs, args.eta, args.batch)
    return 0

if __name__ == "__main__":
    args = ArgumentParser().parse()
    dataParser = DataParser(sys.argv[1])
    exit(main(dataParser, args))
